#!/usr/bin/env python3

"""
Tool to build (or build upon) a Seamless FAIR dir.

FAIR dirs contain collections in deepcell/deepfolder format. 
fairdir-build creates these collections from a source directory that contains the original files.

FAIR dirs are served by the FAIR servers that support querying by collection name and other metadata.

Collections may overlap, i.e. a file checksum can go into different collections.

Collections have formats (e.g. gzipped) and may also have URL info where to download each
individual checksum.

Every time fairdir-build is run, a collection is updated. Unchanged checksums from 
the existing collections are re-used.
"""

import os
import json
import orjson
import csv
from ruamel.yaml import YAML

yaml = YAML(typ="safe")
import time
import re
import shutil
import subprocess
import multiprocessing
import concurrent.futures
import copy
import gc
from seamless import Checksum, Buffer
from seamless.util import cson
from silk.mixed import MAGIC_SEAMLESS_MIXED, MAGIC_NUMPY
from seamless.checksum.buffer_cache import buffer_cache
from seamless.checksum.convert import try_convert
from seamless.checksum.buffer_info import BufferInfo, convert_from_buffer_info
from seamless.checksum.cached_calculate_checksum import (
    calculate_checksum_cache,
    checksum_cache,
)


calculate_checksum_cache.disable()
checksum_cache.disable()
from seamless.checksum.serialize import serialize_cache

serialize_cache.disable()
from seamless.checksum.deserialize import deserialize_cache

deserialize_cache.disable()


def err(*args, **kwargs):
    print("ERROR: " + args[0], *args[1:], **kwargs)
    exit(1)


def report(*args_, **kwargs):
    if not args.verbose:
        return
    print(*args_, **kwargs)


def write_csv(d: dict, filename):
    with open(filename, "w", newline="") as csvfile:
        csvwriter = csv.writer(
            csvfile, delimiter=" ", quotechar="|", quoting=csv.QUOTE_MINIMAL
        )
        for key, value in d.items():
            if isinstance(value, list):
                csvwriter.writerow([key] + value)
            elif isinstance(value, tuple):
                csvwriter.writerow([key] + list(value))
            else:
                csvwriter.writerow([key, value])


def load_csv(filename):
    result = {}
    with open(filename, newline="") as csvfile:
        spamreader = csv.reader(csvfile, delimiter=" ", quotechar="|")
        for row in spamreader:
            if len(row) == 2:
                result[row[0]] = row[1]
            else:
                result[row[0]] = row[1:]
    return result


def get_plain_buffer(d: dict):
    return Buffer(d, "plain").value


def write_json(d: dict, filename):
    with open(filename, "wb") as f:
        f.write(get_plain_buffer(d))


def remove_stale_buffer_hardlink(buffer_dir, checksum, inode):
    buffer_file = os.path.join(buffer_dir, checksum)
    if os.path.exists(buffer_file):
        buffer_inode = str(os.stat(buffer_file).st_ino)
        if buffer_inode == inode:
            report("Remove stale buffer hardlink {}".format(buffer_file))
            os.remove(buffer_file)


def intern_buffer(buffer_dir, checksum, filename, hardlink):
    buffer_file = os.path.join(buffer_dir, checksum)
    if os.path.exists(buffer_file):
        # We trust that the buffer is correct...
        return
    if hardlink:
        os.link(filename, buffer_file)
    else:
        shutil.copyfile(filename, buffer_file)


def get_buffer_info(checksum):
    return buffer_infos.get(checksum)


changed_buffer_info = False


def set_buffer_info(buffer_info: BufferInfo):
    global changed_buffer_info
    buffer_infos[checksum] = buffer_info
    changed_buffer_info = True


def write_buffer_length(checksum, length):
    buffer_info = get_buffer_info(checksum)
    if buffer_info is None:
        buffer_info = BufferInfo(checksum)
    if buffer_info.length != length:
        buffer_info.length = length
        set_buffer_info(buffer_info)


def update_buffer_info(buffer_info: BufferInfo, attr, value):
    # Adapted from buffer_cache.update_buffer_info
    co_flags = {
        "is_json": ("is_utf8",),
        "json_type": ("is_json",),
        "is_json_numeric_array": ("is_json",),
        "is_json_numeric_scalar": ("is_json",),
        "dtype": ("is_numpy",),
        "shape": ("is_numpy",),
    }
    anti_flags = {
        "is_json": ("is_numpy", "is_seamless_mixed"),
        "is_numpy": ("is_json", "is_seamless_mixed"),
        "is_seamless_mixed": ("is_json", "is_numpy"),
    }

    # if attr == "is_numpy":
    #    assert value == False
    # if attr == "json_type":
    #    print("B!", buffer_info.as_dict())
    buffer_info[attr] = value
    if value:
        for f in co_flags.get(attr, []):
            update_buffer_info(buffer_info, f, True)
        for f in anti_flags.get(attr, []):
            update_buffer_info(buffer_info, f, False)
    elif value == False:
        for f in co_flags.get(attr, []):
            update_buffer_info(buffer_info, f, False)
    # if attr == "json_type":
    #    print("BB!", buffer_info.as_dict())
    #    exit(0)


def do_operation(cmd, source_celltype, target_celltype, ops):
    # TODO:
    # For cmd, try to synthesize a TransformationJob:
    #  e.g. bash transformer "ln -s inp inp.gz && gunzip -c inp.gz"
    # re-use existing TransformationJob results from jobs/, and write them also!
    # TODO: for some reason, this causes massive memory leaks,
    # even if all Seamless caches are cleaned and emptied.
    # For now, just clean up the executor every 1000 jobs
    result = []
    for checksum, filename in ops:
        if cmd is not None:
            proc = subprocess.run(cmd + [filename], stdout=subprocess.PIPE, text=False)
            if proc.returncode != 0:
                if proc.returncode == -2:
                    raise KeyboardInterrupt
                print("Error in operationing {}".format(filename))
            buf = proc.stdout
        else:
            with open(filename, "rb") as f:
                buf = f.read()
        buf_checksum = Buffer(buf).get_checksum().value
        if source_celltype != target_celltype:
            source_buf = buf
            source_checksum = buf_checksum

            old_source_buffer_info = get_buffer_info(source_checksum)
            source_buffer_info = copy.deepcopy(old_source_buffer_info)
            if source_buffer_info is None:
                source_buffer_info = BufferInfo(source_checksum)
                source_buffer_info.length = len(source_buf)
            buffer_cache.buffer_info[bytes.fromhex(source_checksum)] = (
                source_buffer_info
            )

            try:
                source_buf.decode()
                source_text = True
            except Exception:
                source_text = False
            update_buffer_info(source_buffer_info, "is_utf8", source_text)

            cs = bytes.fromhex(source_checksum)
            result_buf = None
            conversion_result = try_convert(
                cs,
                source_celltype,
                target_celltype,
                buffer_info=source_buffer_info,
                buffer=source_buf,
            )
            if conversion_result == True:
                result_checksum = source_checksum
            elif isinstance(conversion_result, bytes):
                result_checksum = conversion_result.hex()
            else:
                value = Buffer(source_buf, checksum=source_checksum).deserialize(
                    source_celltype
                )
                result_buf = Buffer(value, target_celltype)
                result_checksum = result_buf.get_checksum()
                result_buf = result_buf.value

            if result_checksum != source_checksum:
                if result_buf is None:
                    result_buf = buffer_cache.get_buffer(result_checksum)
                assert result_buf is not None
            else:
                result_buf = buf

            old_result_buffer_info = get_buffer_info(result_checksum)
            result_buffer_info = copy.deepcopy(old_result_buffer_info)
            if result_buffer_info is None:
                result_buffer_info = BufferInfo(result_checksum)
                result_buffer_info.length = len(result_buf)
            result_buffer_info2 = buffer_cache.buffer_info.get(
                bytes.fromhex(result_checksum)
            )
            if result_buffer_info2 is not None:
                result_buffer_info.update(result_buffer_info2)

            if source_buffer_info != old_source_buffer_info:
                set_buffer_info(source_buffer_info)

            if result_buffer_info != old_result_buffer_info:
                set_buffer_info(result_buffer_info)
        else:
            old_buffer_info = get_buffer_info(buf_checksum)
            buffer_info = copy.deepcopy(old_buffer_info)
            if buffer_info is None:
                buffer_info = BufferInfo(buf_checksum)
                buffer_info.length = len(buf)
            try:
                buf.decode()
                buf_text = True
            except Exception:
                buf_text = False
            update_buffer_info(buffer_info, "is_utf8", buf_text)
            result_buf = buf
            result_checksum = buf_checksum
            if buffer_info != old_buffer_info:
                set_buffer_info(buffer_info)

        buffer_file = os.path.join(BUFFER_DIR, result_checksum)
        if os.path.exists(buffer_file):
            # We trust that the buffer is correct...
            pass
        else:
            assert result_buf is not None
            try:
                with open(buffer_file, "wb") as f:
                    f.write(result_buf)
            except Exception:
                os.remove(buffer_file)
        buffer_cache._uncache_buffer(result_checksum)
        result.append((checksum, result_checksum))
    gc.collect()
    return result


def write_deepcontent(deepdict_checksum, deepdict):
    deep_indices_dir = os.path.join(FAIRDIR, "deep_indices")
    if not os.path.exists(deep_indices_dir):
        os.mkdir(deep_indices_dir)
    buffersizefile = os.path.join(FAIRDIR, "deep_indices", "buffersizes.csv")
    if os.path.exists(buffersizefile):
        buffersizes = load_csv(buffersizefile)
    else:
        buffersizes = {}
    deepcontent_size = 0
    new_checksums = []
    for checksum in deepdict.values():
        if checksum in buffersizes:
            deepcontent_size += int(buffersizes[checksum])
        else:
            new_checksums.append(checksum)
    for n, checksum in enumerate(new_checksums):
        if n > 1 and n % 5000 == 0:
            report("Calculating deep content size: {}/{}".format(n, len(new_checksums)))
        buffer_info = get_buffer_info(checksum)
        try:
            if buffer_info is None:
                raise Exception(checksum)
            buffersize = buffer_info.length
            if buffersize is None:
                raise Exception(checksum, buffer_info.as_dict())
        except Exception:
            buffer_file = os.path.join(BUFFER_DIR, checksum)
            with open(buffer_file, "rb") as f:
                buffer = f.read()
                buffersize = len(buffer)
                buffer_info = BufferInfo(checksum)
                update_buffer_info(buffer_info, "length", buffersize)
        deepcontent_size += buffersize
        buffersizes[checksum] = buffersize
    if len(new_checksums):
        write_csv(buffersizes, buffersizefile)
    report(
        "Write deep content size for {}: {}".format(deepdict_checksum, deepcontent_size)
    )
    deepcontent_file = os.path.join(FAIRDIR, "deep_indices", "deepcontent.csv")
    if os.path.exists(deepcontent_file):
        deepcontent = load_csv(deepcontent_file)
    else:
        deepcontent = {}
    deepcontent[deepdict_checksum] = deepcontent_size
    write_csv(deepcontent, deepcontent_file)


def get_subdir(subdirname):
    subdir = os.path.join(FAIRDIR, subdirname)
    if not os.path.exists(subdir):
        os.mkdir(subdir)
        report("Created {}".format(subdir))
    if not os.path.isdir(subdir):
        err("{} is not a directory".format(subdir))
    return subdir


import argparse

parser = argparse.ArgumentParser()
parser.add_argument(
    "fairdir", help="Seamless FAIR directory to build or update", type=str
)
parser.add_argument(
    "actionfile",
    help="FAIR dir action file (json, cson or yaml) to run",
    type=argparse.FileType("r"),
)

parser.add_argument(
    "--source",
    help="Directory that will be indexed into a collection",
    type=str,
    required=True,
)

parser.add_argument(
    "--bufferdir",
    help="Seamless buffer dir where to store (hardlinks to) the buffers in the collections",
    type=str,
    required=True,
)

parser.add_argument(
    "--collection-bufferdir",
    help="""Seamless buffer dir where to store the buffers of the collection objects (deep buffers).
These are normally much smaller than the total size of the buffers in the collection.
If not specified, collection objects are written to "bufferdir".""",
    type=str,
)

parser.add_argument("--verbose", "-v", action="store_true")
parser.add_argument("--force", "-f", action="store_true")

args = parser.parse_args()

FAIRDIR = args.fairdir
if not os.path.isdir(FAIRDIR):
    err(
        "Target FAIR directory '{}' does not exist. It must at least exist as an empty directory.".format(
            FAIRDIR
        )
    )

dirpath = args.source
if not dirpath:
    err(
        "--source must point to an existing directory that will be indexed into a collection"
    )
if not os.path.exists(dirpath):
    err("{} does not exist".format(dirpath))
if not os.path.isdir(dirpath):
    err("{} is not a directory".format(dirpath))

BUFFER_DIR = args.bufferdir
if not os.path.isdir(BUFFER_DIR):
    err("Buffer directory '{}' does not exist".format(BUFFER_DIR))

COLLECTION_BUFFER_DIR = args.collection_bufferdir
if COLLECTION_BUFFER_DIR is None:
    COLLECTION_BUFFER_DIR = BUFFER_DIR
else:
    if not os.path.isdir(COLLECTION_BUFFER_DIR):
        err(
            "Collection buffer directory '{}' does not exist".format(
                COLLECTION_BUFFER_DIR
            )
        )

afname = args.actionfile.name
if afname.endswith(".json"):
    loader = json.load
elif afname.endswith(".cson"):
    
    def loader(file):
        return cson.cson2json(file.read())
    
elif afname.endswith(".yaml"):

    def loader(file):
        return yaml.load(file.read())

else:
    err("actionfile must be .json, .cson or .yaml")

try:
    actiondict = loader(args.actionfile)
except Exception as exc:
    err("could not parse actionfile")

report("Loaded actionfile:")
report(json.dumps(actiondict, indent=2))


# TODO: probably run actions against a JSON schema...
ok = False
if isinstance(actiondict, dict):
    while 1:
        if "collection" not in actiondict:
            break
        if "cross_device" not in actiondict:
            break
        if "actions" not in actiondict:
            break
        ok = True
        break
if not ok:
    err(
        "malformatted actionfile: 'collection', 'cross_device' and 'actions' are required"
    )
# /TODO


default_collection_name = actiondict["collection"]
cross_device = actiondict["cross_device"]

buffer_infos = {}
buffer_info_file = os.path.join(FAIRDIR, "buffer_info.json")
if os.path.exists(buffer_info_file):
    report("Existing buffer info file found. Loading...")
    with open(buffer_info_file) as f:
        buf = f.read()
    buffer_infos0 = orjson.loads(buf)
    for cs, d in buffer_infos0.items():
        buffer_infos[cs] = BufferInfo(cs, d)
    report("Existing buffer info file loaded")
else:
    report("No existing buffer info file found.")

# Build inode table.
# If there is an old inode table:
#   remove interned buffers that point to an inode that has changed
inode_dir = get_subdir("inodes")
inode_table_file = os.path.join(inode_dir, default_collection_name) + ".csv"
try:
    old_inode_table = load_csv(inode_table_file)
except Exception as exc:
    old_inode_table = {}

inode_table = {}
inode_to_entry = {}
nfiles = 0
inodes_changed = False
last_inode_write = time.time()
for root, dirnames, filenames in os.walk(dirpath, followlinks=False):
    nfiles += len(filenames)
for root, dirnames, filenames in sorted(os.walk(dirpath, followlinks=False)):
    for filename in filenames:
        filepath = os.path.join(root, filename)
        if not root.startswith(dirpath):  # would be weird...
            report("Error in {}".format(filepath))
            continue
        entry = os.path.join(root[len(dirpath) :], filename).lstrip("/")
        try:
            stat = os.stat(filepath, follow_symlinks=False)
            inode = str(stat.st_ino)
            mtime = str(stat.st_mtime)
        except OSError:
            report("Error in {}".format(filepath))
            continue

        inode_to_entry[inode] = entry
        from_cache = False
        if inode in old_inode_table:
            v = old_inode_table[inode]
            if isinstance(v, list) and len(v) == 2:
                old_checksum, old_mtime = v
                if mtime == old_mtime:
                    inode_table[inode] = v
                    from_cache = True

        if not from_cache:
            try:
                with open(filepath, "rb") as f:
                    content = f.read()
            except OSError:
                report("Error in {}".format(filepath))
                continue
            checksum = Buffer(content).get_checksum().value
            write_buffer_length(checksum, len(content))
            if inode in old_inode_table:
                # If there is a buffer hardlink with the old checksum
                #  that points to the inode, clean it up
                old_checksum, _ = old_inode_table[inode]
                remove_stale_buffer_hardlink(BUFFER_DIR, old_checksum, inode)
            inode_table[inode] = [checksum, mtime]
            inodes_changed = True

            if len(inode_table) % 1000 == 0:
                report(
                    "Calculated checksums for {}/{} files".format(
                        len(inode_table), nfiles
                    )
                )
        elapsed_time = time.time()
        if elapsed_time - last_inode_write > 20:
            report("Save intermediate inode table to {}".format(inode_table_file))
            curr_inode_table = old_inode_table.copy()
            curr_inode_table.update(inode_table)
            write_csv(curr_inode_table, inode_table_file)
            last_inode_write = elapsed_time

if not inodes_changed:
    if args.force:
        report("Inode table unchanged, but execution is forced")
    else:
        print(
            """Inode table unchanged. 
Unless the action file was modified, there is nothing to do. 
Use --force to force execution"""
        )
        exit(0)
else:
    report("Write inode table to {}".format(inode_table_file))
    write_csv(inode_table, inode_table_file)


report("Running actions")
collections = {
    default_collection_name: {"name": default_collection_name, "default": True}
}
actions = actiondict.get("actions", [])

executor = concurrent.futures.ProcessPoolExecutor()
try:
    for action in actions:
        name = action["action"]

        title = action.get("collection")
        if title is None:
            title = action.get("source_collection", default_collection_name)
        elif "source_collection" in action:
            title = action["source_collection"] + " => " + title
        report('Action {} "{}"'.format(name, title))
        if name == "intern_collection":
            source_collection_name = action.get(
                "source_collection", default_collection_name
            )
            source_collection = collections[source_collection_name]
            hardlink = action["hardlink"]
            if source_collection.get("default"):
                if hardlink and cross_device:
                    # TODO: do such checks before running any actions
                    err("Cross-device directory cannot be interned with hardlinks")
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    entry = inode_to_entry[inode]
                    filename = os.path.join(dirpath, entry)
                    intern_buffer(BUFFER_DIR, checksum, filename, hardlink)

            elif source_collection.get("copied"):
                raise NotImplementedError  # TODO: make a test for this
            else:
                msg = 'Action "intern_collection": source collection "{}" must be default or copied'
                err(msg.format(source_collection_name))

            source_collection["interned"] = True

        elif name == "operation":
            nproc = max(int(multiprocessing.cpu_count() / 2), 1)
            target_collection_name = action["collection"]
            assert target_collection_name not in collections, target_collection_name
            command = action.get("command")
            assert command in ("gunzip", "unzip", "bunzip2", None)
            if command == "gunzip":
                cmd = ["gunzip", "-c"]
            elif command == "bunzip2":
                cmd = ["bunzip2", "-c"]
            elif command == "unzip":
                cmd = ["unzip", "-p"]
            else:
                cmd = None
            target_celltype = action.get("celltype")
            source_collection_name = action.get(
                "source_collection", default_collection_name
            )
            source_collection = collections[source_collection_name]
            operation_dir = get_subdir("operations")
            operation_file = (
                os.path.join(operation_dir, target_collection_name) + ".csv"
            )
            if os.path.exists(operation_file):
                old_operation = load_csv(operation_file)
            else:
                old_operation = {}

            operation = {}
            ops = []
            noperation = 0
            if source_collection.get("default"):
                origin_collection_name = default_collection_name
                origin_collection = collections[default_collection_name]
                source_celltype = "bytes"
                done_checksums = set()
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    if checksum in old_operation:
                        result_checksum = old_operation[checksum]
                        result_checksum_file = os.path.join(BUFFER_DIR, result_checksum)
                        if os.path.exists(result_checksum_file):
                            noperation += 1
                            operation[checksum] = result_checksum
                            continue
                    if checksum in done_checksums:
                        continue
                    noperation += 1
                    done_checksums.add(checksum)
                    entry = inode_to_entry[inode]
                    op = os.path.join(dirpath, entry)
                    ops.append((checksum, op))
            elif source_collection.get("from_operation"):
                raise NotImplementedError  # TODO: make a test for this
                origin_collection_name = source_collection["origin"]
                origin_collection = collections[origin_collection_name]
                source_celltype = source_collection.get("celltype", "bytes")
            elif source_collection.get("copied"):
                origin_collection = source_collection
                origin_collection_name = source_collection["name"]
                source_celltype = source_collection.get("celltype", "bytes")
                collection_dir = get_subdir("collections")
                source_collection_dir = os.path.join(
                    collection_dir, source_collection_name
                )
                result_file = (
                    os.path.join(collection_dir, source_collection_name) + ".csv"
                )
                result = load_csv(result_file)
                done_checksums = set()
                for entry, checksum in result.items():
                    if checksum in old_operation:
                        result_checksum = old_operation[checksum]
                        result_checksum_file = os.path.join(BUFFER_DIR, result_checksum)
                        if os.path.exists(result_checksum_file):
                            noperation += 1
                            operation[checksum] = result_checksum
                            continue
                    if checksum in done_checksums:
                        continue
                    noperation += 1
                    done_checksums.add(checksum)
                    op = os.path.join(source_collection_dir, entry)
                    ops.append((checksum, op))
            else:
                raise AssertionError(source_collection)

            futures = []
            chunksize = 30
            last_operation_write = time.time()
            curr_ops = []
            assert source_celltype is not None
            if target_celltype is None:
                target_celltype = source_celltype
            for n in range(len(ops)):
                op = ops[n]
                curr_ops.append(op)
                last = n == len(ops) - 1
                cleanup = False
                if (n + 1) % 1000 == 0:
                    if source_celltype != target_celltype:
                        cleanup = True
                if len(curr_ops) == chunksize or last:
                    future = executor.submit(
                        do_operation, cmd, source_celltype, target_celltype, curr_ops
                    )
                    futures.append(future)
                    curr_ops = []
                while len(futures) and (len(futures) == nproc or last or cleanup):
                    for future in futures:
                        if not future.done():
                            continue
                        result = future.result()
                        futures.remove(future)
                        for checksum, result_checksum in result:
                            if result_checksum is not None:
                                operation[checksum] = result_checksum
                                if len(operation) % 1000 == 0:
                                    report(
                                        "Operated {}/{} files".format(
                                            len(operation), noperation
                                        )
                                    )

                                elapsed_time = time.time()
                                if elapsed_time - last_operation_write > 20:
                                    report(
                                        "Save intermediate operation table to {}".format(
                                            operation_file
                                        )
                                    )
                                    write_csv(operation, operation_file)
                                    last_operation_write = elapsed_time
                        break
                    else:
                        time.sleep(1)
                if cleanup:
                    executor.shutdown()
                    executor = concurrent.futures.ProcessPoolExecutor()
            assert not len(futures)
            write_csv(operation, operation_file)
            collections[target_collection_name] = {
                "name": target_collection_name,
                "from_operation": True,
                "celltype": target_celltype,
                "origin": origin_collection_name,
            }

        elif name == "copy_collection":
            source_collection_name = action.get(
                "source_collection", default_collection_name
            )
            source_collection = collections[source_collection_name]
            target_collection_name = action["collection"]
            assert target_collection_name not in collections, target_collection_name
            collection_dir = get_subdir("collections")
            target_collection_dir = os.path.join(collection_dir, target_collection_name)
            interned = False
            can_hardlink = False
            if source_collection.get("default"):
                celltype = None
                origin_collection_name = default_collection_name
                origin_collection = collections[origin_collection_name]
                can_hardlink = cross_device
                if origin_collection.get("interned"):
                    interned = True
            elif source_collection.get("from_operation"):
                origin_collection_name = source_collection["origin"]
                origin_collection = collections[origin_collection_name]
                can_hardlink = True
                interned = True
                celltype = source_collection["celltype"]
                if celltype == "bytes":
                    celltype = None
            elif source_collection.get("copied"):
                origin_collection_name = source_collection_name
                origin_collection = source_collection
                can_hardlink = True
                interned = True
                celltype = source_collection.get("celltype")
                if celltype == "bytes":
                    celltype = None
            else:
                raise AssertionError(source_collection)
            hardlink = action["hardlink"]
            if hardlink and not can_hardlink:
                msg = 'Action "copy_collection": source collection "{}" cannot be hardlinked'
                err(msg.format(source_collection_name))

            raw_filenames = {}
            origin_checksums = {}
            if origin_collection.get("default"):
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    entry = inode_to_entry[inode]
                    origin_checksums[entry] = checksum
                    raw_filenames[entry] = os.path.join(dirpath, entry)
            elif origin_collection.get("copied"):
                collection_dir = get_subdir("collections")
                origin_collection_dir = os.path.join(
                    collection_dir, origin_collection["name"]
                )
                result_file = (
                    os.path.join(collection_dir, origin_collection_name) + ".csv"
                )
                result = load_csv(result_file)
                deepcell = {}
                for entry, checksum in result.items():
                    origin_checksums[entry] = checksum
                    raw_filenames[entry] = os.path.join(origin_collection_dir, entry)
            else:
                raise AssertionError(origin_collection)

            if source_collection.get("from_operation"):
                operation_dir = get_subdir("operations")
                operation_file = (
                    os.path.join(operation_dir, source_collection_name) + ".csv"
                )
                operation = load_csv(operation_file)
                checksums = {
                    entry: operation[checksum]
                    for entry, checksum in origin_checksums.items()
                }
            else:
                checksums = origin_checksums

            if interned:
                filenames = {
                    entry: os.path.join(BUFFER_DIR, checksum)
                    for entry, checksum in checksums.items()
                }
            else:
                filenames = raw_filenames

            result = {}
            regex = re.compile(action["source_file"])
            target_file_template = action["target_file"]
            to_copy = []
            count = 0

            for source_entry, checksum in checksums.items():
                if count > 0 and count % 50000 == 0:
                    report("Checking {}/{} files".format(count, len(checksums)))
                count += 1
                filename = filenames[source_entry]
                unnamed_capturing_groups = regex.match(source_entry).groups()
                named_capturing_groups = regex.match(source_entry).groupdict()
                target_entry = target_file_template.format(
                    *unnamed_capturing_groups, **named_capturing_groups
                )
                result[target_entry] = checksum

                target_file = os.path.join(target_collection_dir, target_entry)
                if os.path.exists(target_file):
                    old_ino = os.stat(target_file).st_ino
                    if hardlink:
                        ino = os.stat(filename).st_ino
                        if old_ino == ino:
                            continue
                    buf_file = os.path.join(BUFFER_DIR, checksum)
                    if os.path.exists(buf_file):
                        ino = os.stat(buf_file).st_ino
                        if old_ino == ino:
                            continue
                    os.remove(target_file)
                to_copy.append((filename, target_file))
            for n in range(len(to_copy)):
                filename, target_file = to_copy[n]
                if n > 0 and n % 20000 == 0:
                    term = "Hardlink" if hardlink else "Copy"
                    report("{} {}/{} files".format(term, n, len(to_copy)))
                os.makedirs(os.path.dirname(target_file), exist_ok=True)
                if hardlink:
                    os.link(filename, target_file)
                else:
                    shutil.copyfile(filename, target_file)

            result_file = os.path.join(collection_dir, target_collection_name) + ".csv"
            old_result = {}
            if os.path.exists(result_file):
                old_result = load_csv(result_file)
            for target_entry in old_result:
                if target_entry not in result:
                    target_file = os.path.join(target_collection_dir, target_entry)
                    if os.path.exists(target_file):
                        os.remove(target_file)
            write_csv(result, result_file)
            collections[target_collection_name] = {
                "name": target_collection_name,
                "copied": True,
            }
            if celltype is not None:
                collections[target_collection_name]["celltype"] = celltype

        elif name == "build_download_index":
            source_file_or_key = "source_file"
            source_collection_name = action.get(
                "source_collection", default_collection_name
            )
            source_collection = collections[source_collection_name]
            if source_collection.get("default"):
                entries = inode_to_entry.values()
            elif source_collection.get("copied"):
                collection_dir = get_subdir("collections")
                result_file = (
                    os.path.join(collection_dir, source_collection_name) + ".csv"
                )
                result = load_csv(result_file)
                entries = result.keys()
            elif source_collection.get("deepcell"):
                source_file_or_key = "source_key"
                deepcell_dir = get_subdir("deepcells")
                deepcell_file = (
                    os.path.join(deepcell_dir, target_collection_name) + ".json"
                )
                deepcell = json.load(open(deepcell_file))
                entries = deepcell.keys()
            else:
                msg = 'Action "build_download_index": source collection "{}" must be default, copied or deepcell'
                err(msg.format(source_collection_name))

            download_index = {}
            urls = action["urls"]
            regex = re.compile(action[source_file_or_key])
            for entry in entries:
                print(entry)
                unnamed_capturing_groups = regex.match(entry).groups()
                named_capturing_groups = regex.match(entry).groupdict()
                target_urls = []
                for urldict in urls:
                    if isinstance(urldict, str):
                        urldict = {"url": urldict}
                    target = {}
                    for attr in "compression", "celltype":
                        if attr in urldict:
                            target[attr] = urldict[attr]
                    target["url"] = urldict["url"].format(
                        *unnamed_capturing_groups, **named_capturing_groups
                    )
                    target_urls.append(target)
                download_index[entry] = target_urls
            download_index_dir = get_subdir("download_indices")
            download_index_file = (
                os.path.join(download_index_dir, source_collection_name) + ".json"
            )
            report("Write download index to {}".format(download_index_file))
            write_json(download_index, download_index_file)

        elif name == "deepfolder":
            source_collection_name = action.get(
                "source_collection", default_collection_name
            )
            source_collection = collections[source_collection_name]
            deepfolder_dir = get_subdir("deepfolders")
            deepfolder_file = (
                os.path.join(deepfolder_dir, source_collection_name) + ".json"
            )
            if os.path.exists(deepfolder_file):
                old_deepfolder = json.load(open(deepfolder_file))
                old_deepfolder_buf = get_plain_buffer(old_deepfolder)
                old_deepfolder_checksum = (
                    Buffer(old_deepfolder_buf).get_checksum().value
                )
                old_deepfolder_inode = str(os.stat(deepfolder_file).st_ino)
            else:
                old_deepfolder_checksum = None
            deepfolder = {}
            if source_collection.get("default"):
                for inode in inode_table:
                    checksum, _ = inode_table[inode]
                    entry = inode_to_entry[inode]
                    deepfolder[entry] = checksum
            elif source_collection.get("copied"):
                collection_dir = get_subdir("collections")
                result_file = (
                    os.path.join(collection_dir, target_collection_name) + ".csv"
                )
                result = load_csv(result_file)
                deepfolder = result
            else:
                msg = 'Action "deepfolder": source collection "{}" must be default or copied'
                err(msg.format(source_collection_name))

            deepfolder_buf = get_plain_buffer(deepfolder)
            deepfolder_checksum = Buffer(deepfolder_buf).get_checksum().value
            print(
                "Computed deepfolder for {}: deep buffer checksum {}".format(
                    source_collection_name, deepfolder_checksum
                )
            )
            if (
                old_deepfolder_checksum is not None
                and deepfolder_checksum != old_deepfolder_checksum
            ):
                remove_stale_buffer_hardlink(
                    COLLECTION_BUFFER_DIR, old_deepfolder_checksum, old_deepfolder_inode
                )
            report("Write deepfolder deep buffer to {}".format(deepfolder_file))
            write_json(deepfolder, deepfolder_file)
            intern_buffer(
                COLLECTION_BUFFER_DIR,
                deepfolder_checksum,
                deepfolder_file,
                hardlink=True,
            )
            write_deepcontent(deepfolder_checksum, deepfolder)

        elif name == "deepcell":
            source_collection_name = action.get(
                "source_collection", default_collection_name
            )
            target_collection_name = action["collection"]
            assert target_collection_name not in collections
            source_collection = collections[source_collection_name]
            deepcell_dir = get_subdir("deepcells")
            deepcell_file = os.path.join(deepcell_dir, target_collection_name) + ".json"
            if os.path.exists(deepcell_file):
                old_deepcell = json.load(open(deepcell_file))
                old_deepcell_buf = get_plain_buffer(old_deepcell)
                old_deepcell_checksum = Buffer(old_deepcell_buf).get_checksum().value
                old_deepcell_inode = str(os.stat(deepcell_file).st_ino)
            else:
                old_deepcell_checksum = None
            deepcell = {}
            if source_collection.get("default"):
                celltype = None
            elif source_collection.get("copied"):
                celltype = source_collection.get("celltype")
            elif source_collection.get("from_operation"):
                celltype = source_collection.get("celltype")
            else:
                celltype = None

            if celltype != "mixed":
                msg = 'Action "deepcell": source collection "{}" must have celltype "mixed"'
                err(msg.format(source_collection_name))

            regex = re.compile(action["source_file"])
            target_key_template = action["target_key"]

            if source_collection.get("copied"):
                raise NotImplementedError  # TODO: a test for this
                collection_dir = get_subdir("collections")
                result_file = (
                    os.path.join(collection_dir, target_collection_name) + ".csv"
                )
                result = load_csv(result_file)
                deepcell0 = result
            elif source_collection.get("from_operation"):
                operation_dir = get_subdir("operations")
                operation_file = (
                    os.path.join(operation_dir, source_collection_name) + ".csv"
                )
                operation = load_csv(operation_file)
                origin_collection_name = source_collection["origin"]
                origin_collection = collections[origin_collection_name]

                if origin_collection.get("default"):
                    origin_checksums = {}
                    deepcell0 = {}
                    for inode in inode_table:
                        checksum, _ = inode_table[inode]
                        entry = inode_to_entry[inode]
                        deepcell0[entry] = operation[checksum]
                elif origin_collection.get("copied"):
                    collection_dir = get_subdir("collections")
                    result_file = (
                        os.path.join(collection_dir, origin_collection_name) + ".csv"
                    )
                    result = load_csv(result_file)
                    deepcell0 = {}
                    for entry, checksum in result.items():
                        deepcell0[entry] = operation[checksum]
                else:
                    raise AssertionError(origin_collection)
            else:
                raise AssertionError(source_collection)

            deepcell = {}
            for entry, checksum in deepcell0.items():
                try:
                    unnamed_capturing_groups = regex.match(entry).groups()
                    named_capturing_groups = regex.match(entry).groupdict()
                except AttributeError:
                    raise ValueError(entry) from None
                target_entry = target_key_template.format(
                    *unnamed_capturing_groups, **named_capturing_groups
                )
                deepcell[target_entry] = checksum

            deepcell_buf = get_plain_buffer(deepcell)
            deepcell_checksum = Buffer(deepcell_buf).get_checksum().value
            print(
                "Computed deepcell for {} -> {}: deep buffer checksum {}".format(
                    source_collection_name, target_collection_name, deepcell_checksum
                )
            )
            if (
                old_deepcell_checksum is not None
                and deepcell_checksum != old_deepcell_checksum
            ):
                remove_stale_buffer_hardlink(
                    COLLECTION_BUFFER_DIR, old_deepcell_checksum, old_deepcell_inode
                )
            report("Write deepcell deep buffer to {}".format(deepcell_file))
            write_json(deepcell, deepcell_file)
            intern_buffer(
                COLLECTION_BUFFER_DIR, deepcell_checksum, deepcell_file, hardlink=True
            )
            write_deepcontent(deepcell_checksum, deepcell)
            collections[target_collection_name] = {
                "name": target_collection_name,
                "origin": source_collection_name,
                "deepcell": True,
            }
        if changed_buffer_info:
            buf = orjson.dumps(
                {k: v.as_dict() for k, v in buffer_infos.items()}
            ).decode()
            with open(buffer_info_file, "w") as f:
                f.write(buf)
            changed_buffer_info = False
finally:
    executor.shutdown()
