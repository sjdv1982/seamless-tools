#!/usr/bin/env python3

"""
Saves the current state of a FAIR directory collection (deepcell or deepfolder) as a distribution.
Distributions must be marked with a date and/or version.
Distributions of the same collection at different points in time form a dataset.

This follows the terminology of FAIR datapoint (https://specs.fairdatapoint.org/fdp-specs-v1.2.html).
In the future, fairdir-build and fairserver will be FAIR datapoint compliant.

fairdir-add-distribution will add three new subdirectories (directly served by fairserver) to a FAIR dir,
if they don't exist already:

- distributions. Contains for each dataset a JSON file which contains a list of distributions

- access_index. Contains the list of download URLs for each item in the distribution. 
Essentially a snapshot of a file in download_indices/. 
This is per-distribution, but in practice, this is often maintained 
per-dataset, as the index is rather large and is more often incremented instead of replaced.

- keyorder. Checksum-to-content dir for the ordered keys of every distribution.
Keys are ordered by the checksum of each key, which guarantees a random ordering.
"""

import os
import csv
import sys
import json
from seamless import Checksum, Buffer
from seamless.util.cson import cson2json


def err(*args, **kwargs):
    print("ERROR: " + args[0], *args[1:], **kwargs, file=sys.stderr)
    exit(1)


def load_csv(filename):
    result = {}
    with open(filename, newline="") as csvfile:
        spamreader = csv.reader(csvfile, delimiter=" ", quotechar="|")
        for row in spamreader:
            if len(row) == 2:
                result[row[0]] = row[1]
            else:
                result[row[0]] = row[1:]
    return result


import argparse

parser = argparse.ArgumentParser(
    description="""Takes a snapshot of a collection in a FAIR dir,
adding it as a distribution of a FAIR dataset, tagging it with a version or date.
Distributions can be served by the fairserver.
"""
)

parser.add_argument("fairdir", help="Seamless FAIR directory to update", type=str)

parser.add_argument("dataset_name", help="Name of the FAIR dataset")

parser.add_argument(
    "collection_name",
    help="Collection name in FAIRDIR. Must be a deepcell or deepfolder",
)

parser.add_argument(
    "--version",
    help="Version tag of the added distribution. Only required if there is no date.",
)

parser.add_argument(
    "--date",
    help="Date tag of the added distribution. Only required if there is no version.",
)

parser.add_argument(
    "--format",
    help="""Format tag of the added distribution. 
This is just descriptive, allowing the same collection to be in the same FAIR dataset
multiple times, with distributions for multiple different file formats.
""",
)

parser.add_argument(
    "--compression",
    help="""Compression that describes the deepfolder. Can be gzip, zip or bzip.
Seamless does not automatically decompress such deepfolders, but command line tools
may expect a directory containing compressed files.
""",
)

parser.add_argument(
    "--merge-access-index",
    dest="merge_access_index",
    action="store_true",
    help="""Merge access indices (download lists) with the current latest distribution.
If set, access index (if it exists) will be merged with existing access index 
in FAIRDIR for the distribution that is latest  (given the specified format 
and compression).
If not, a new access index is built.""",
)

parser.add_argument(
    "--no-latest",
    dest="no_latest",
    action="store_true",
    help="""Do not mark the added distribution as "latest". 
The latest distribution is what is returned to a FAIRserver request that
does not explicitly describe version or date.""",
)

args = parser.parse_args()

FAIRDIR = args.fairdir
if not os.path.isdir(FAIRDIR):
    err("Target FAIR directory '{}' does not exist.".format(FAIRDIR))

deepcontent_file = os.path.join(FAIRDIR, "deep_indices", "deepcontent.csv")
if not os.path.exists(deepcontent_file):
    err(
        f"Deep content file '{deepcontent_file}' does not exist. This doesn't seem a proper FAIR directory, or it doesn't contain any proper collections."
    )

SUBDIRECTORIES = "distributions", "access_index", "keyorder"
for subdir in SUBDIRECTORIES:
    subdir_path = os.path.join(FAIRDIR, subdir)
    if not os.path.exists(subdir_path):
        os.mkdir(subdir_path)

if args.date is None and args.version is None:
    err("You must define --date and/or --version")

if args.compression not in (None, "gzip", "zip", "bzip"):
    err("If defined, compression must be gzip, zip or bzip")

deep_buffer_file1 = os.path.join(FAIRDIR, "deepcells", args.collection_name + ".json")
deep_buffer_file2 = os.path.join(FAIRDIR, "deepfolders", args.collection_name + ".json")
if not os.path.exists(deep_buffer_file1) and not os.path.exists(deep_buffer_file2):
    err(
        """Collection name does not exist or is not a deepcell or deepfolder.
Neither file exists:
  {}
  {}""".format(
            deep_buffer_file1, deep_buffer_file2
        )
    )

if os.path.exists(deep_buffer_file1) and os.path.exists(deep_buffer_file2):
    err(
        """Malformed FAIR directory structure.
Both files exist:
  {}
  {}""".format(
            deep_buffer_file1, deep_buffer_file2
        )
    )

# Get deep buffer from $FAIRDIR/<deepcell-or-deepfolder>/<collection_name>.json.
# Calculate checksum.

if os.path.exists(deep_buffer_file1):
    deep_buffer_file = deep_buffer_file1
    distribution_type = "deepcell"
else:
    deep_buffer_file = deep_buffer_file2
    distribution_type = "deepfolder"
with open(deep_buffer_file, "rb") as f:
    deep_buffer = f.read()
deep_buffer_size = len(deep_buffer)
deep_buffer_checksum = Buffer(deep_buffer).get_checksum().value
deep_buffer_dict = json.loads(deep_buffer.decode())
distribution_nkeys = len(deep_buffer_dict)

# Get deep buffer content length from deepcontent.csv
deepcontent = load_csv(deepcontent_file)
content_size = int(deepcontent[deep_buffer_checksum])


# If exists, load existing distributions from $FAIRDIR/distributions/<dataset_name>
distributions_file = os.path.join(FAIRDIR, "distributions", args.dataset_name + ".json")
if os.path.exists(distributions_file):
    with open(distributions_file) as f:
        # Load with CSON to be more robust towards manual editing
        distributions = cson2json(f.read())
        assert isinstance(distributions, list)
else:
    distributions = []

# Check that there is no distribution with the exact same version and date
for n, distribution in enumerate(distributions.copy()):
    if distribution["type"] != distribution_type:
        continue
    if distribution.get("format") != args.format:
        continue
    if distribution.get("compression") != args.compression:
        continue
    if distribution.get("version") != args.version:
        continue
    if distribution.get("date") != args.date:
        continue
    err("Distribution {} has the exact same version/date".format(n + 1))

# Get all compatible distributions (same type, format and compression)
compatible_distributions = []
for distribution in distributions:
    if distribution["type"] != distribution_type:
        continue
    if distribution.get("format") != args.format:
        continue
    if distribution.get("compression") != args.compression:
        continue
    compatible_distributions.append(distribution)

# Get the latest distribution from the compatible distributions
for distribution in compatible_distributions:
    if not distribution.get("latest"):
        continue
    latest_distribution = distribution
    break
else:
    latest_distribution = None

"""
Build the keyorder. Take the keyorder from the latest distribution as basis.
Then add/delete new keys from the current distribution (sorted by key checksum, i.e. randomly). 
To maximize chunk coherence, fill up deleted keys:
  - preferably, with a new key from the current distribution
  - else, with an existing key from the latest distribution
Add all remaining new keys from the current distribution to the end.
Convert the keyorder list to Seamless JSON.
Calculate the checksum of the keyorder buffer and store the buffer in 
$FAIRDIR/keyorder/<key order checksum>
"""
print("Determine key order...")
latest_keyorder = []
if latest_distribution is not None:
    latest_keyorder_checksum = latest_distribution["keyorder"]
    with open(os.path.join(FAIRDIR, "keyorder", latest_keyorder_checksum)) as f:
        latest_keyorder = json.load(f)
latest_keyorder_set = set(latest_keyorder)

sorted_keys = sorted(
    deep_buffer_dict.keys(), key=lambda k: Buffer(k, "str").get_checksum()
)
added_keys = [key for key in sorted_keys if key not in latest_keyorder_set]

keyorder = []
added_key_pos = 0
latest_keyorder_size = len(latest_keyorder)
pos = 0
while pos < latest_keyorder_size:
    key = latest_keyorder[pos]
    pos += 1
    if key in deep_buffer_dict:
        keyorder.append(key)
    else:
        if added_key_pos < len(added_keys):
            added_key = added_keys[added_key_pos]
            keyorder.append(added_key)
            added_key_pos += 1
        else:
            final_key = None
            while pos < latest_keyorder_size - 1:
                final_key = latest_keyorder[latest_keyorder_size - 1]
                latest_keyorder_size -= 1
                if final_key in deep_buffer_dict:
                    break
            if final_key is not None:
                keyorder.append(final_key)
keyorder += added_keys[added_key_pos:]

keyorder_buffer = Buffer(keyorder, "plain")
keyorder_checksum = keyorder_buffer.get_checksum().value
keyorder_buffer = keyorder_buffer.value
print("Key order checksum: {}".format(keyorder_checksum))
with open(os.path.join(FAIRDIR, "keyorder", keyorder_checksum), "wb") as f:
    f.write(keyorder_buffer)

# If exists, load $FAIRDIR/download_indices/<collection_name>.json as the raw access index

raw_access_index = None
raw_access_index_file = os.path.join(
    FAIRDIR, "download_indices", args.collection_name + ".json"
)
if os.path.exists(raw_access_index_file):
    with open(raw_access_index_file, "r") as f:
        raw_access_index = json.load(f)

"""
If --merge-access-index, update the last access index.
Else, make a brand new access index.

The access index is built from the raw access index above
 by converting each of its key to a checksum using the deep buffer
Once the access index has been built, calculate its checksum and store it under 
 $FAIRDIR/access_index/<access-index-checksum>
"""

access_index = {}
access_index_checksum = None
if raw_access_index is not None:

    latest_access_index_checksum = None
    if args.merge_access_index and latest_distribution is not None:
        latest_access_index_checksum = latest_distribution.get("access_index")
        if latest_access_index_checksum is not None:
            latest_access_index_file = os.path.join(
                FAIRDIR, "access_index", latest_access_index_checksum
            )
            with open(latest_access_index_file) as f:
                latest_access_index = json.load(f)
            access_index = latest_access_index

    print("Building access index...")
    for key, value in raw_access_index.items():
        if key not in deep_buffer_dict:
            print("WARNING: unknown key {}".format(key))
            continue
        checksum = deep_buffer_dict[key]
        access_index[checksum] = value
    access_index_buffer = Buffer(access_index, "plain")
    access_index_checksum = access_index_buffer.get_checksum().value
    access_index_buffer = access_index_buffer.value
    print("Access index checksum: {}".format(access_index_checksum))
    with open(os.path.join(FAIRDIR, "access_index", access_index_checksum), "wb") as f:
        f.write(access_index_buffer)

    """If --merge-access-index, delete the old access index.
    Any distributions that point to it, will use the new index instead
    """
    if latest_access_index_checksum is not None:
        for distribution in compatible_distributions:
            if distribution.get("access_index") == latest_access_index_checksum:
                distribution["access_index"] = access_index_checksum
        os.remove(latest_access_index_file)

"""
Build the distribution and add it to the previously loaded distributions in
$FAIRDIR/distributions/<dataset_name> . An distribution contains essentially what is served, but in addition it contains the list of raw access index files.
"""
if latest_distribution is not None:
    latest_distribution.pop("latest")

# TODO: JSON-LD support (schemas.org)
new_distribution = {
    "checksum": deep_buffer_checksum,
    "type": distribution_type,
    "version": args.version,
    "date": args.date,
    "format": args.format,
    "compression": args.compression,
    "latest": None if args.no_latest else True,
    "nkeys": distribution_nkeys,
    "index_size": deep_buffer_size,
    "content_size": content_size,
    "keyorder": keyorder_checksum,
}
if access_index_checksum is not None:
    new_distribution["access_index"] = access_index_checksum

for key in list(new_distribution.keys()):
    if not new_distribution[key]:
        new_distribution.pop(key)

print(json.dumps(new_distribution, indent=2))
distributions.append(new_distribution)
with open(distributions_file, "w") as f:
    json.dump(distributions, f, indent=2)
